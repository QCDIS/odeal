{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the initial and the unlabeled sets for pool-based AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/randomsplit/train\"\n",
    "n_initial = 100\n",
    "\n",
    "split_method = 'random'\n",
    "# split_method = 'ocsvm'\n",
    "# split_method = 'lof'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ratio(dataset):\n",
    "    ''' Compute anomaly ratio\n",
    "    '''\n",
    "    instance = dataset[(dataset['Label']==1)]\n",
    "    rate=len(instance)/len(dataset)*100\n",
    "    return round(rate,2), len(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PATH = \"../data/randomtrain/\"\n",
    "# TEST_PATH = \"../data/randomtest\"\n",
    "\n",
    "# float_number = '3901684'\n",
    "# float_number = '4903217'\n",
    "# float_number = '4902919'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the CSV file path and the number of initial samples\n",
    "# csv_file = os.path.join(TRAIN_PATH, f'PR_PF_{float_number}.csv')\n",
    "\n",
    "# # Call the function to split the dataset\n",
    "# split_dataset(csv_file, n_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for all floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(split_method, csv_file, n_initial):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    data_df = pd.read_csv(csv_file)\n",
    "    error_ratio, _ = comp_ratio(data_df)\n",
    "    error_ratio = error_ratio/100\n",
    "\n",
    "    if split_method=='random': \n",
    "        # Randomly select n_initial samples\n",
    "        initial_set = data_df.sample(n=n_initial, random_state=42)\n",
    "\n",
    "    elif split_method=='ocsvm':\n",
    "        n_seed = int(n_initial/2)\n",
    "        k = n_initial - n_seed\n",
    "        subset1 = data_df.sample(n=n_seed, random_state=42)\n",
    "        subset2 = data_df.drop(subset1.index)\n",
    "        normal_data = subset1[subset1['Label']==0]\n",
    "        \n",
    "        X = normal_data.drop(columns=['ID', 'Date', 'Label'])  # Replace 'label_column' with the actual label column name\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Fit the OC-SVM model\n",
    "        ocsvm = OneClassSVM(nu=0.01, kernel='rbf', gamma='scale')  # You can adjust parameters as needed\n",
    "        ocsvm.fit(X_scaled)\n",
    "\n",
    "        Z = subset2.drop(columns=['ID', 'Date', 'Label'])\n",
    "\n",
    "        # Standardize the features\n",
    "        Z_scaled = scaler.fit_transform(Z)\n",
    "\n",
    "        # Predict anomaly scores for instances\n",
    "        anomaly_scores = ocsvm.decision_function(Z_scaled)\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = anomaly_scores.argsort()[:k]\n",
    "        \n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = pd.concat([subset1, subset2.iloc[top_k_anomalies_indices]])\n",
    "\n",
    "    elif split_method=='lof': \n",
    "        # Drop any columns that are not features\n",
    "        X = data_df.drop(columns=['ID', 'Date', 'Label'])  # Replace 'label_column' with the actual label column name\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Fit the LOF model\n",
    "        lof = LocalOutlierFactor(n_neighbors=2, contamination=error_ratio)  # You can adjust parameters as needed\n",
    "        labels = lof.fit_predict(X_scaled)\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = labels.argsort()[:n_initial]\n",
    "\n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = data_df.iloc[top_k_anomalies_indices]\n",
    "\n",
    "    elif split_method=='iforest': \n",
    "        # Fit the Isolation Forest model\n",
    "        isoforest = IsolationForest(contamination=0.1)  # You can adjust parameters as needed\n",
    "        isoforest.fit(X_scaled)\n",
    "\n",
    "        # Predict anomaly scores (negative scores indicate anomalies)\n",
    "        anomaly_scores = isoforest.decision_function(X_scaled)\n",
    "\n",
    "        # Transform anomaly scores into probability-like values\n",
    "        # Using sigmoid function to map scores to [0, 1]\n",
    "        prob_scores = 1 / (1 + np.exp(-anomaly_scores))\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = prob_scores.argsort()[:n_initial]\n",
    "\n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = data_df.iloc[top_k_anomalies_indices]\n",
    "\n",
    "    else: \n",
    "        print('Sorry the split method is not supported. (´-ω-`)')\n",
    "        return\n",
    "    \n",
    "    # Get the remaining samples for the unlabeled set\n",
    "    unlabeled_set = data_df.drop(initial_set.index)\n",
    "    \n",
    "    # Get the directory of the input CSV file\n",
    "    csv_dir = os.path.dirname(csv_file)\n",
    "    \n",
    "    # Get the base filename without the extension\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    \n",
    "    # Save the initial set and unlabeled set in the same directory\n",
    "    initial_set.to_csv(os.path.join(csv_dir, f'{split_method}_{base_filename}_{n_initial}_initial.csv'), index=False)\n",
    "    unlabeled_set.to_csv(os.path.join(csv_dir, f'{split_method}_{base_filename}_{n_initial}_unlabeled.csv'), index=False)\n",
    "    \n",
    "    print(f\"{split_method} split: {error_ratio} errors, {n_initial} initial samples, {initial_set.Label.sum()} anomalies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Float: 4903220 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 1 anomalies.\n"
     ]
    }
   ],
   "source": [
    "float_numbers = [\n",
    "    '4903052',\n",
    "    '4903054',\n",
    "    '4903058',\n",
    "    '4903215',\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220'\n",
    "]\n",
    "\n",
    "float_numbers = [\n",
    "    '4903220'\n",
    "]\n",
    "\n",
    "for float_number in float_numbers: \n",
    "    # Specify the CSV file path and the number of initial samples\n",
    "    csv_file = os.path.join(TRAIN_PATH, f'PR_PF_{float_number}.csv')\n",
    "    \n",
    "    # Call the function to split the dataset\n",
    "    print(f'------ Float: {float_number} ------')\n",
    "    split_dataset(split_method, csv_file, n_initial)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-quality-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
